{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.Y_train = np.array(y)\n",
    "        \n",
    "    def compute_distance(self, x, X):\n",
    "        x = np.array(x) \n",
    "        X = np.array(X)\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            squared_diffs = (X - x) ** 2\n",
    "            summed_squared_diffs = np.sum(squared_diffs, axis=1)\n",
    "            distances = np.sqrt(summed_squared_diffs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "        return distances\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict continuous probabilities for the test data.\"\"\"\n",
    "        probabilities = []\n",
    "        for x in X:\n",
    "            distances = self.compute_distance(x, self.X_train)\n",
    "            k_nearest_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.Y_train[k_nearest_indices]  # Keep floats intact\n",
    "            # Calculate the mean (or weighted average) of nearest labels\n",
    "            prob = np.mean(k_nearest_labels)  # Use mean for continuous probabilities\n",
    "            probabilities.append(prob)  # Assume binary case for simplicity\n",
    "\n",
    "        return np.array(probabilities)\n",
    "    def get_exit_probabilities(knn_model, X_test):\n",
    "        # Ensure X_test is a NumPy array or DataFrame\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.values  # Convert DataFrame to NumPy array\n",
    "        return knn_model.predict_proba(X_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_path, test_path, k=5):\n",
    "    # Load the datasets\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    \n",
    "    train_data = train_data.iloc[:, 3:]  # Keep columns from index 3 onwards\n",
    "    test_data = test_data.iloc[:, 3:]    # Keep columns from index 3 onwards\n",
    "    \n",
    "    train_data = train_data.drop(train_data.columns[[1,2,6,7,8]], axis = 1)\n",
    "    test_data = test_data.drop(test_data.columns[[1,2,6,7,8]], axis = 1)\n",
    "    \n",
    "    # Handle missing values and data type conversions\n",
    "    train_data = handle_missing_values(train_data)\n",
    "    test_data = handle_missing_values(test_data)\n",
    "    \n",
    "    \n",
    "    # Convert appropriate columns to numeric types\n",
    "    numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']  # Update with actual numeric column names\n",
    "    for col in numeric_cols:\n",
    "        train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "        test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('Exited', axis=1)  # Assuming 'Exited' is the label\n",
    "    y_train = train_data['Exited']\n",
    "    X_test = test_data.copy()  # Test data typically has no labels\n",
    "    \n",
    "    #X_train, y_train = smote(X_train.to_numpy(), y_train.to_numpy(), k=5)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    return df\n",
    "\n",
    "def scale_features(train_df, test_df):\n",
    "    numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        mean = train_df[col].mean()\n",
    "        std = train_df[col].std()\n",
    "\n",
    "        # Scale bosth train and test sets using the same statistics\n",
    "        train_df[col] = (train_df[col] - mean) / std\n",
    "        test_df[col] = (test_df[col] - mean) / std\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    fold_size = len(X) // n_splits\n",
    "    indices = np.arange(len(X))  # Use the natural order of indices\n",
    "    auc_scores = []\n",
    "    for i in range(n_splits):\n",
    "        # Define test indices for the current fold\n",
    "        if i == n_splits - 1:  # Last fold\n",
    "            test_idx = indices[i * fold_size:]  # Get remaining samples\n",
    "        else:\n",
    "            test_idx = indices[i * fold_size: (i + 1) * fold_size]  # Normal case\n",
    "        \n",
    "        # Define train indices\n",
    "        train_idx = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n",
    "        \n",
    "        # Use .iloc for DataFrame indexing\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Fit the model and make predictions\n",
    "        knn.fit(X_train.values, y_train.values)\n",
    "        y_pred = knn.predict_proba(X_test.values)\n",
    "\n",
    "        # Compute AUC score and append to scores list\n",
    "        auc = compute_roc_auc(y_test.values, y_pred)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    return auc_scores\n",
    "\n",
    "def compute_roc_auc(y_true, y_prob):\n",
    "    # Sort by predicted probabilities\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    y_true = y_true[sorted_indices]\n",
    "    y_prob = y_prob[sorted_indices]\n",
    "\n",
    "    # Compute true positive and false positive rates\n",
    "    tpr = np.cumsum(y_true) / np.sum(y_true)  # True Positive Rate\n",
    "    fpr = np.cumsum(1 - y_true) / np.sum(1 - y_true)  # False Positive Rate\n",
    "\n",
    "    # Compute the AUC using the trapezoidal rule\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=45, Mean AUC=0.8229\n",
      "k=46, Mean AUC=0.8237\n",
      "k=47, Mean AUC=0.8237\n",
      "k=48, Mean AUC=0.8238\n",
      "k=49, Mean AUC=0.8245\n",
      "k=50, Mean AUC=0.8248\n",
      "k=51, Mean AUC=0.8252\n",
      "k=52, Mean AUC=0.8257\n",
      "k=53, Mean AUC=0.8258\n",
      "k=54, Mean AUC=0.8264\n",
      "k=55, Mean AUC=0.8268\n",
      "k=56, Mean AUC=0.8271\n",
      "k=57, Mean AUC=0.8271\n",
      "k=58, Mean AUC=0.8267\n",
      "k=59, Mean AUC=0.8274\n",
      "k=60, Mean AUC=0.8274\n",
      "k=61, Mean AUC=0.8277\n",
      "k=62, Mean AUC=0.8277\n",
      "k=63, Mean AUC=0.8283\n",
      "k=64, Mean AUC=0.8279\n",
      "k=65, Mean AUC=0.8280\n",
      "k=66, Mean AUC=0.8283\n",
      "k=67, Mean AUC=0.8286\n",
      "k=68, Mean AUC=0.8284\n",
      "k=69, Mean AUC=0.8287\n",
      "k=70, Mean AUC=0.8287\n",
      "k=71, Mean AUC=0.8287\n",
      "k=72, Mean AUC=0.8287\n",
      "k=73, Mean AUC=0.8287\n",
      "k=74, Mean AUC=0.8290\n",
      "k=75, Mean AUC=0.8287\n",
      "k=76, Mean AUC=0.8286\n",
      "k=77, Mean AUC=0.8294\n",
      "k=78, Mean AUC=0.8289\n",
      "k=79, Mean AUC=0.8291\n",
      "k=80, Mean AUC=0.8292\n",
      "k=81, Mean AUC=0.8292\n",
      "k=82, Mean AUC=0.8295\n",
      "k=83, Mean AUC=0.8290\n",
      "k=84, Mean AUC=0.8295\n",
      "k=85, Mean AUC=0.8296\n",
      "k=86, Mean AUC=0.8293\n",
      "k=87, Mean AUC=0.8297\n",
      "k=88, Mean AUC=0.8295\n",
      "k=89, Mean AUC=0.8296\n",
      "k=90, Mean AUC=0.8296\n",
      "k=91, Mean AUC=0.8296\n",
      "k=92, Mean AUC=0.8297\n",
      "k=93, Mean AUC=0.8298\n",
      "k=94, Mean AUC=0.8297\n",
      "k=95, Mean AUC=0.8297\n",
      "k=96, Mean AUC=0.8297\n",
      "k=97, Mean AUC=0.8298\n",
      "k=98, Mean AUC=0.8299\n",
      "k=99, Mean AUC=0.8298\n",
      "Best k: 98 with AUC: 0.8299\n",
      "Test predictions saved to 'submissions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, X_test, y = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def hyperparameter_tuning(X, y, k_values=[i for i in range(45,100)]):\n",
    "    best_k = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNN(k=k, distance_metric='euclidean')\n",
    "        scores = cross_validate(X, y, knn, n_splits=5)\n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"k={k}, Mean AUC={mean_score:.4f}\")\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    print(f\"Best k: {best_k} with AUC: {best_score:.4f}\")\n",
    "    return best_k\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "optimal_k = hyperparameter_tuning(X, y)\n",
    "\n",
    "# Train final model with optimal hyperparameters\n",
    "knn = KNN(k=optimal_k, distance_metric='euclidean')\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = knn.get_exit_probabilities(X_test)\n",
    "\n",
    "# Save test predictions\n",
    "test_data = pd.read_csv('test.csv')  # Load the test file to get the 'id' column\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'Exited': test_predictions\n",
    "})\n",
    "submission.to_csv('submissions.csv', index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'submissions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
